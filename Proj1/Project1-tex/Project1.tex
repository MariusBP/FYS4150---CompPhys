\documentclass[10pt,showpacs,preprintnumbers,footinbib,amsmath,amssymb,aps,prl,twocolumn,groupedaddress,superscriptaddress,showkeys]{revtex4-1}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{color}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{hyperref}

\begin{document}



\title[CPP1]{Computational Physics Project 1}

\author{Marius B. Pran}
\affiliation{Institute of Theoretical Astrophysics, University of Oslo}
\author{Espen Hodne} 
\affiliation{Institute of Theoretical Astrophysics, University of Oslo}


\begin{abstract}
We compare three different methods of solving matrix systems, as described by i.e. Poisson's equation. We use a generalized program, an optimized program, and LU decomposition, and compare error margins and runtimes between the three methods. In general the program behaved as expected, with longer runtimes and smaller errors for larger matrices. The most interesting result comes from comparing the runtime of the general and optimized method using the same matrix size. For a matrix of size $10^6\times10^6$, the general method runs for $1.44576$ seconds while the optimized runs for $1.40901$ seconds. However, for a $10^8\times10^8$ matrix, the general program runs in $141.9$ seconds while the optimized program runs in $141.753$ seconds. If we do not write out the results to a file, the processes now take respectively $41.3648$ and $39.9341$ seconds. This clearly shows that optimizing the program for a single case works, but it might not be worth the extra work required in this particular case. The LU decomposition proved a vastly inferior method in terms of speed, though this might be an issue with the computer memory required and not the method itself. 
\end{abstract}



\maketitle



\section{Introduction}

In this project we are solving matrix and array equations numerically. Specifically, we look at equations of the form 

$$
\mathbf{\hat{A}} \mathbf{u} = \mathbf{f},
$$

where $\mathbf{\hat{A}}$ is a known tridiagonal matrix, $\mathbf{f}$ is a known vector, and $\mathbf{x}$ is an unknown vector.


In physics, we often use Poisson's equation $\nabla^2\Phi = -4\pi\rho(r)$, for example in gravitation and electromagnetism. Therefore, we want a quick, effective way of solving it numerically. This can be done very efficiently by setting it up as a tridiagonal matrix. This should use much less memory than brute-forcing it. Our main motivation is to write a program which is able to quickly and efficiently solve these tridiagonal matrices, in order to apply it later to solving physical problems.


We are also using the C++ library Armadillo, in order to compare it to our own methods. We will specifically see how memory allocation differs. We are using LU-decomposition from Armadillo, and comparing it to our mathematical method (which is outlined in detail in the "methods" section.

In this case we are specifically looking at the case where $\mathbf{\hat{A}}$ has the values 2 in the middle diagonal, and -1 in the side diagonals. Using this special case we should be able to solve large matrices very quickly with our method.

\section{Methods and algortihms}

The main focus in this project is to take a look at how a computer handles solving second-order differential equations on the form 
\begin{equation*}
\frac{d^2y}{dx^2}+k^2(x)y = f(x),
\end{equation*}
and write a faster code which we will use to solve a special case of the equation.

The special case we are looking at is the so-called Poisson's equation 
\begin{equation*}
\nabla^2 \Phi = -4\pi \rho (\mathbf{r}),
\end{equation*}
which consists of a potential $\Phi$ and a distribution $\rho$, usually concerning charge or other potentials.

In the special case when the potential is spherically symmetric along with the distribution, the equation can be simplified to one dimension, since it is equal in all other directions and only the distance $r$ from the center matters. The equation simplifies to 
\begin{equation*}
\frac{d^2\phi}{dr^2}= -4\pi r\rho(r),
\end{equation*}
which we will write as 
\begin{equation*}
-u''(x) = f(x)
\end{equation*}

while looking at the area $x\in(0,1)$,
and the boundary conditions $u(0) = u(1) = 0$.

Since we are solving this numerically, we need to approximate the second derivative with 

\begin{equation*}
   -\frac{v_{i+1}+v_{i-1}-2v_i}{h^2} = f_i
\end{equation*}
for i between 1 and n. 
With each point being separated by a distance $h = 1/(n+1)$. $v$ is the approximation we use for the exact solution $u$. Writing the equation this way allows us to rewrite the entire problem as a linear set of equations on the form 
\begin{equation*}
   \mathbf{\hat{A}}\mathbf{u} = \mathbf{f},
\end{equation*}

with the matrix $\mathbf{\hat{A}}$ being a tridiagonal $n\times n$ matrix on the form
\[
    \mathbf{\hat{A}} = \begin{bmatrix}
                           2& -1& 0 &\dots   & \dots &0 \\
                           -1 & 2 & -1 &0 &\dots &\dots \\
                           0&-1 &2 & -1 & 0 & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           0&\dots   &  &-1 &2& -1 \\
                           0&\dots    &  & 0  &-1 & 2 \\
                      \end{bmatrix},
\]
and $\mathbf{u_i} = h^2 f(x_i)$.

In the code, we just call $h^2 f(x_i)$ for $f$.

We are given a function 
\begin{equation*}
f = 100e^{-10x}
\end{equation*}

and an exact solution to this is
\begin{equation*}
u = 1 - 1(1 - 10e^{-10})x - e^{-10x}.
\end{equation*}
which we can easily see as 
\begin{equation*}
-\frac{d^2u}{dx^2} = f
\end{equation*}
Given these, we write a program to compute the numerical solution to this and find the error
\[
   \epsilon_i=log_{10}\left(\left|\frac{v_i-u_i}
                 {u_i}\right|\right),
\]

After doing this we find a specialized algorithm where
\begin{equation*}
\tilde{d} = \frac{i+1}{i}
\end{equation*}
and we simply insert the values for a and the initial value for d to get a new algoritm for $\tilde{f}$
\begin{equation*}
\tilde{f_i} = f_i + \tilde{f}_{i-1}/\tilde{d}_{i-1}
\end{equation*}
and for the backwards sumstitution for $v$
\begin{equation*}
v_{i-1} = (\tilde{f}_{i-1} + v_{i})/\tilde{d}_{i-1}.
\end{equation*}

Specialising the code this way reduces the FLOPS needed from about $9n$ for the standard program, to about $4n$ for the specialized code.


\section{Results}

The program gives us the run times and error margins for the three methods we have used, as well as plots showing the calculated function, and comparisions to the exact solution. The three methods is a the generalized program, the specialized program, and finally Armadillo's LU decomposition. We plot for $n = 10$, $100$ and $1000$, and make runtime and error calculations for $n = 10^6$. The exception is for the LU decomposition, where $n = 10^4$.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{standard_n10.png}
		\caption{n = 10, general program}
	\end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{optimized_n10.png}
		\caption{n = 10, specialized program}
	\end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{LU_n10.png}
		\caption{n = 10, LU decomposition}
	\end{subfigure}
    \caption{All three methods for n = 10}
    \label{figure:n10}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{standard_n100.png}
		\caption{n = 100, general program}
	\end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{optimized_n100.png}
		\caption{n = 100, specialized program}
	\end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{LU_n100.png}
		\caption{n = 100, LU decomposition}
	\end{subfigure}
    \caption{All three methods for n = 100}
    \label{figure:n100}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{standard_n1000.png}
		\caption{n = 1000, general program}
	\end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{optimized_n1000.png}
		\caption{n = 1000, specialized program}
	\end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{LU_n1000.png}
		\caption{n = 1000, LU decomposition}
	\end{subfigure}
    \caption{All three methods for n = 1000}
    \label{figure:n1000}
\end{figure}

\begin{table}[]
\centering
\caption{Runtime and error margins for different numerical solutions}
\begin{tabular}{l|l|l|l}
Method           & n      & Error Margin        & Run Time[s] \\
\hline
General          & $10^6$ & $7.68\cdot10^{-6}$  & 1.44576 \\
			 & $10^7$ & $1.35\cdot10^{-7}$  & 14.8502  \\
Optimized        & $10^6$ & 0                   & 1.40901 \\
LU Decomposition & $10^4$ & $6.95\cdot 10^{-4}$ & 206.57 
\end{tabular}
\label{table:errors}
\end{table}
The plots for $n = 10$ can be seen in FIG. \ref{figure:n10}. The same is true for $n = 100$ and $n = 1000$, which can be seen in FIG. \ref{figure:n100} and FIG. \ref{figure:n1000} respectively.

Table \ref{table:errors} shows the error margins and run times of the three methods.

\section{Conclusions and perspectives}
Looking at the results above, we can see that although the generalized and LU-decomposition algorithms work fine, they are a lot more taxing on the computer than the specialized algorithm. Additionally, the specialized solution is a lot more accurate and faster than the other algorithms due to the added presicion since we need to make less approximations. 

While the LU-decomposition works well, it is really taxing on the computer memory since it needs to save excessively large matrices to solve the problem. This means that not every computer can run this since the PC will quickly crash once it runs out of memory. Our computers could only handle $n$s up to about $10^4$, and even then it took a long time to run as seen in Table\ref{table:errors}.  While the tridiagonal simplification to three vectors is less taxing memory-wise, it still uses about twice as many FLOPS as the specialized solution.

Looking at the errors of the different algorithms compared to the exact solution in Table \ref{table:errors}, we can see that the optimized solution has an estimated error of $0$. This is probably not exact since we need to account for numerical precision, but the precision is a lot higher on the specialized solution regardless, since it has to be lower than $10^{-7}$, which our code should be able to show. Also in Table \ref{table:errors}, we can see the error when we ran the program with $n = 10^{-7}$. While the error is lower than what it was with $n = 10^{6}$, this is the lower border of numerical precision. This means that should we reduce the step-length further the error will probably be larger, since the computer can't handle numbers this small. 

For large, complex problems like these we would probably have to find some sort of specialized solution to simplify the algorithms, otherwise computing a solution to it will be extremely difficult or time consuming, probably requiring a supercomputer. Especially for the LU-composition, which takes several orders of magnitude longer to run, not including the large amount of memory it needs in addition. 

\section{Extra Material}
You can find the code we used to calculate these results at: \url{https://github.com/MariusBP/FYS4150---CompPhys/tree/master/Proj1}

\begin{thebibliography}{99}
\bibitem{jensen} M.~H.̃~Jensen, Computational~Physics(11.09.2017) \url{github.com/CompPhysics/ComputationalPhysics/blob/master/doc/Projects/2017/ReportExamplesLatexstyle/reportexample.tex}
\end{thebibliography}

\end{document}